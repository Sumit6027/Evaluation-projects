{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03463805",
   "metadata": {},
   "source": [
    "# Census Income Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916147db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Statement:\n",
    "This data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over $50K a year.\n",
    "\n",
    "Description of fnlwgt (final weight)\n",
    "The weights on the Current Population Survey (CPS) files are controlled to independent estimates of the civilian non-institutional population of the US. These are prepared monthly for us by Population Division here at the Census Bureau. We use 3 sets of controls. These are:\n",
    "\n",
    "A single cell estimate of the population 16+ for each state.\n",
    "\n",
    "Controls for Hispanic Origin by age and sex.\n",
    "\n",
    "Controls by Race, age and sex.\n",
    "\n",
    "We use all three sets of controls in our weighting program and \"rake\" through them 6 times so that by the end we come back to all the controls we used. The term estimate refers to population totals derived from CPS by creating \"weighted tallies\" of any specified socio-economic characteristics of the population. People with similar demographic characteristics should have similar weights. There is one important caveat to remember about this statement. That is that since the CPS sample is actually a collection of 51 state samples, each with its own probability of selection, the statement only applies within state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ca925",
   "metadata": {},
   "source": [
    "### Importing require library for performing EDA, Data Wrangling and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b8db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for data wrangling purpose\n",
    "import numpy as np # Basic computation library\n",
    "import seaborn as sns # For Visualization \n",
    "import matplotlib.pyplot as plt # ploting package\n",
    "%matplotlib inline\n",
    "import warnings # Filtering warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb190e5b",
   "metadata": {},
   "source": [
    "Importing CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cd2887",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('census_income.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a55f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No of Rows:',df.shape[0])\n",
    "print('No of Columns:',df.shape[1])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5254a797",
   "metadata": {},
   "source": [
    "No of Rows: 32560\n",
    "No of Columns: 15\n",
    "Age\tWorkclass\tFnlwgt\tEducation\tEducation_num\tMarital_status\tOccupation\tRelationship\tRace\tSex\tCapital_gain\tCapital_loss\tHours_per_week\tNative_country\tIncome\n",
    "0\t50\tSelf-emp-not-inc\t83311\tBachelors\t13\tMarried-civ-spouse\tExec-managerial\tHusband\tWhite\tMale\t0\t0\t13\tUnited-States\t<=50K\n",
    "1\t38\tPrivate\t215646\tHS-grad\t9\tDivorced\tHandlers-cleaners\tNot-in-family\tWhite\tMale\t0\t0\t40\tUnited-States\t<=50K\n",
    "2\t53\tPrivate\t234721\t11th\t7\tMarried-civ-spouse\tHandlers-cleaners\tHusband\tBlack\tMale\t0\t0\t40\tUnited-States\t<=50K\n",
    "3\t28\tPrivate\t338409\tBachelors\t13\tMarried-civ-spouse\tProf-specialty\tWife\tBlack\tFemale\t0\t0\t40\tCuba\t<=50K\n",
    "4\t37\tPrivate\t284582\tMasters\t14\tMarried-civ-spouse\tExec-managerial\tWife\tWhite\tFemale\t0\t0\t40\tUnited-States\t<=50K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4542b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd01f7",
   "metadata": {},
   "source": [
    "Index(['Age', 'Workclass', 'Fnlwgt', 'Education', 'Education_num',\n",
    "       'Marital_status', 'Occupation', 'Relationship', 'Race', 'Sex',\n",
    "       'Capital_gain', 'Capital_loss', 'Hours_per_week', 'Native_country',\n",
    "       'Income'],\n",
    "      dtype='object')\n",
    "      Comment:\n",
    "The Most columns names are self-explanatory remaining are explain below :\n",
    "\n",
    "Fnlwgt: sampling weight\n",
    "Education_num: number of years of education in total\n",
    "Capital_gain/Capital_loss: income from investment sources other than salary/wages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d06df1",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c862eb",
   "metadata": {},
   "source": [
    " Before Going for Statistical exploration of data, first check integrity of data & Missing value\n",
    "\n",
    "### Data Integrity Check\n",
    "Since dataset is large, Let check for any entry which is repeated or duplicated in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdfa34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum() # This will check if any duplicate entry or duplicate row with same value exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14094337",
   "metadata": {},
   "source": [
    "24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa61ece",
   "metadata": {},
   "source": [
    "### If we just check CSV File we can find that there are some missing value in dataset which shown fill with '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c307756",
   "metadata": {},
   "outputs": [],
   "source": [
    "Let check how many question mark (\" ?\") inside dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isin([' ?']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3bb01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Age                  0\n",
    "Workclass         1836\n",
    "Fnlwgt               0\n",
    "Education            0\n",
    "Education_num        0\n",
    "Marital_status       0\n",
    "Occupation        1843\n",
    "Relationship         0\n",
    "Race                 0\n",
    "Sex                  0\n",
    "Capital_gain         0\n",
    "Capital_loss         0\n",
    "Hours_per_week       0\n",
    "Native_country     583\n",
    "Income               0\n",
    "dtype: int64\n",
    "Let replace ' ?' with np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61633843",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.replace(' ?',np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc44ad",
   "metadata": {},
   "source": [
    "#### Let check if any whitespace, 'NA' or '-' exist in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isin([' ','NA','-']).sum().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b4dd8a",
   "metadata": {},
   "source": [
    "False\n",
    "Comment:\n",
    "We have Replace ' ?' with np.NaN\n",
    "No whitespace, NA, '-' exist in dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141e51ee",
   "metadata": {},
   "source": [
    "### Let drop duplicated entry from dataset before checking null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep='last', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d34c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "(32536, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b149b6df",
   "metadata": {},
   "source": [
    "### Missing value check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c7d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11adb42",
   "metadata": {},
   "source": [
    "Comment:\n",
    "There are missing values in Occupation,workclass, Native country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a25d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding what percentage of data is missing from the dataset\n",
    "missing_values = df.isnull().sum().sort_values(ascending = False)\n",
    "percentage_missing_values =(missing_values/len(df))*100\n",
    "print(pd.concat([missing_values, percentage_missing_values], axis =1, keys =['Missing Values', '% Missing data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb93938",
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing Values  % Missing data\n",
    "Occupation                1843        5.664495\n",
    "Workclass                 1836        5.642980\n",
    "Native_country             582        1.788788\n",
    "Income                       0        0.000000\n",
    "Hours_per_week               0        0.000000\n",
    "Capital_loss                 0        0.000000\n",
    "Capital_gain                 0        0.000000\n",
    "Sex                          0        0.000000\n",
    "Race                         0        0.000000\n",
    "Relationship                 0        0.000000\n",
    "Marital_status               0        0.000000\n",
    "Education_num                0        0.000000\n",
    "Education                    0        0.000000\n",
    "Fnlwgt                       0        0.000000\n",
    "Age                          0        0.000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c220f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 32536 entries, 0 to 32559\n",
    "Data columns (total 15 columns):\n",
    " #   Column          Non-Null Count  Dtype \n",
    "---  ------          --------------  ----- \n",
    " 0   Age             32536 non-null  int64 \n",
    " 1   Workclass       30700 non-null  object\n",
    " 2   Fnlwgt          32536 non-null  int64 \n",
    " 3   Education       32536 non-null  object\n",
    " 4   Education_num   32536 non-null  int64 \n",
    " 5   Marital_status  32536 non-null  object\n",
    " 6   Occupation      30693 non-null  object\n",
    " 7   Relationship    32536 non-null  object\n",
    " 8   Race            32536 non-null  object\n",
    " 9   Sex             32536 non-null  object\n",
    " 10  Capital_gain    32536 non-null  int64 \n",
    " 11  Capital_loss    32536 non-null  int64 \n",
    " 12  Hours_per_week  32536 non-null  int64 \n",
    " 13  Native_country  31954 non-null  object\n",
    " 14  Income          32536 non-null  object\n",
    "dtypes: int64(6), object(9)\n",
    "memory usage: 4.0+ MB\n",
    "Observation:\n",
    "There are 32536 rows , 14 Independent columns and 1 Target feature.\n",
    "Age,Fnlwgt, education_num, capital gain, capital loss, hours per week are Numerical variable and having int64 datatypes.\n",
    "Work class, Education,Marital status, occupation, relationship, race,sex,native country are categorical feature with object datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5225b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating numerical and categorical variable\n",
    "Numerical=['Age','Fnlwgt','Education_num','Capital_gain','Capital_loss','Hours_per_week']\n",
    "Category=['Workclass','Education','Marital_status','Occupation','Relationship','Race','Sex','Native_country','Income']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51356510",
   "metadata": {},
   "source": [
    "### Missing value imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616b9e60",
   "metadata": {},
   "source": [
    "#### Occupation, Workclass and Native Country are categorical variable so we can imputate them with mode of that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8425a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputating Missing value with mode for categorical features\n",
    "df['Occupation'].fillna(df['Occupation'].mode()[0],inplace=True)\n",
    "df['Workclass'].fillna(df['Workclass'].mode()[0],inplace=True)\n",
    "df['Native_country'].fillna(df['Native_country'].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b14b4d3",
   "metadata": {},
   "source": [
    "### Missing Value Check After Imputataion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3a93dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding what percentage of data is missing from the dataset\n",
    "missing_values = df.isnull().sum().sort_values(ascending = False)\n",
    "percentage_missing_values =(missing_values/len(df))*100\n",
    "print(pd.concat([missing_values, percentage_missing_values], axis =1, keys =['Missing Values', '% Missing data']))\n",
    "\n",
    "Missing Values  % Missing data\n",
    "Income                       0             0.0\n",
    "Native_country               0             0.0\n",
    "Hours_per_week               0             0.0\n",
    "Capital_loss                 0             0.0\n",
    "Capital_gain                 0             0.0\n",
    "Sex                          0             0.0\n",
    "Race                         0             0.0\n",
    "Relationship                 0             0.0\n",
    "Occupation                   0             0.0\n",
    "Marital_status               0             0.0\n",
    "Education_num                0             0.0\n",
    "Education                    0             0.0\n",
    "Fnlwgt                       0             0.0\n",
    "Workclass                    0             0.0\n",
    "Age                          0             0.0\n",
    "Comment :\n",
    "Finally, No Missing Value is Present.\n",
    "\n",
    "We are Now Yes To Go Further !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494ed967",
   "metadata": {},
   "source": [
    "### Statistical Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566beb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the statistics of the columns using heatmap.\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.describe(),linewidths = 0.1,fmt='0.1f',annot = True,cmap='PiYG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d21919",
   "metadata": {},
   "outputs": [],
   "source": [
    "<AxesSubplot:>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5537da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937568e0",
   "metadata": {},
   "source": [
    "count\tmean\tstd\tmin\t25%\t50%\t75%\tmax\n",
    "Age\t32536.0\t38.585536\t13.638193\t17.0\t28.0\t37.0\t48.00\t90.0\n",
    "Fnlwgt\t32536.0\t189784.298992\t105556.258211\t12285.0\t117831.5\t178356.0\t236993.25\t1484705.0\n",
    "Education_num\t32536.0\t10.081725\t2.571622\t1.0\t9.0\t10.0\t12.00\t16.0\n",
    "Capital_gain\t32536.0\t1078.410069\t7388.068465\t0.0\t0.0\t0.0\t0.00\t99999.0\n",
    "Capital_loss\t32536.0\t87.370912\t403.107737\t0.0\t0.0\t0.0\t0.00\t4356.0\n",
    "Hours_per_week\t32536.0\t40.440343\t12.347079\t1.0\t40.0\t40.0\t45.00\t99.0\n",
    "Observation:\n",
    "The minimum and maximum age of people in the dataset is 19 and 90 years respectively, while the average age is 37.\n",
    "The minimum and maximum years spent on education is 1 and 16 respectively, whereas the mean education level is 10 years.\n",
    "While the minimum and average capital gain is 0, maximum is 99999. This seems odd, maybe some error within the data collection.\n",
    "The number of hours spent per week varies between 1 to 99 and the average being 40 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d80edfb",
   "metadata": {},
   "source": [
    "## Start Exploring categorial features with Enlisting Value counts & Sub-categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0640f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Category:\n",
    "    print(i)\n",
    "    print(df[i].value_counts())\n",
    "    print('='*100)\n",
    "    \n",
    "    Workclass\n",
    " Private             24509\n",
    " Self-emp-not-inc     2540\n",
    " Local-gov            2093\n",
    " State-gov            1297\n",
    " Self-emp-inc         1116\n",
    " Federal-gov           960\n",
    " Without-pay            14\n",
    " Never-worked            7\n",
    "Name: Workclass, dtype: int64\n",
    "====================================================================================================\n",
    "Education\n",
    " HS-grad         10494\n",
    " Some-college     7282\n",
    " Bachelors        5352\n",
    " Masters          1722\n",
    " Assoc-voc        1382\n",
    " 11th             1175\n",
    " Assoc-acdm       1067\n",
    " 10th              933\n",
    " 7th-8th           645\n",
    " Prof-school       576\n",
    " 9th               514\n",
    " 12th              433\n",
    " Doctorate         413\n",
    " 5th-6th           332\n",
    " 1st-4th           166\n",
    " Preschool          50\n",
    "Name: Education, dtype: int64\n",
    "====================================================================================================\n",
    "Marital_status\n",
    " Married-civ-spouse       14970\n",
    " Never-married            10666\n",
    " Divorced                  4441\n",
    " Separated                 1025\n",
    " Widowed                    993\n",
    " Married-spouse-absent      418\n",
    " Married-AF-spouse           23\n",
    "Name: Marital_status, dtype: int64\n",
    "====================================================================================================\n",
    "Occupation\n",
    " Prof-specialty       5979\n",
    " Craft-repair         4094\n",
    " Exec-managerial      4065\n",
    " Adm-clerical         3767\n",
    " Sales                3650\n",
    " Other-service        3291\n",
    " Machine-op-inspct    2000\n",
    " Transport-moving     1597\n",
    " Handlers-cleaners    1369\n",
    " Farming-fishing       992\n",
    " Tech-support          927\n",
    " Protective-serv       649\n",
    " Priv-house-serv       147\n",
    " Armed-Forces            9\n",
    "Name: Occupation, dtype: int64\n",
    "====================================================================================================\n",
    "Relationship\n",
    " Husband           13187\n",
    " Not-in-family      8291\n",
    " Own-child          5064\n",
    " Unmarried          3445\n",
    " Wife               1568\n",
    " Other-relative      981\n",
    "Name: Relationship, dtype: int64\n",
    "====================================================================================================\n",
    "Race\n",
    " White                 27794\n",
    " Black                  3122\n",
    " Asian-Pac-Islander     1038\n",
    " Amer-Indian-Eskimo      311\n",
    " Other                   271\n",
    "Name: Race, dtype: int64\n",
    "====================================================================================================\n",
    "Sex\n",
    " Male      21774\n",
    " Female    10762\n",
    "Name: Sex, dtype: int64\n",
    "====================================================================================================\n",
    "Native_country\n",
    " United-States                 29734\n",
    " Mexico                          639\n",
    " Philippines                     198\n",
    " Germany                         137\n",
    " Canada                          121\n",
    " Puerto-Rico                     114\n",
    " El-Salvador                     106\n",
    " India                           100\n",
    " Cuba                             95\n",
    " England                          90\n",
    " Jamaica                          81\n",
    " South                            80\n",
    " China                            75\n",
    " Italy                            73\n",
    " Dominican-Republic               70\n",
    " Vietnam                          67\n",
    " Japan                            62\n",
    " Guatemala                        62\n",
    " Poland                           60\n",
    " Columbia                         59\n",
    " Taiwan                           51\n",
    " Haiti                            44\n",
    " Iran                             43\n",
    " Portugal                         37\n",
    " Nicaragua                        34\n",
    " Peru                             31\n",
    " Greece                           29\n",
    " France                           29\n",
    " Ecuador                          28\n",
    " Ireland                          24\n",
    " Hong                             20\n",
    " Cambodia                         19\n",
    " Trinadad&Tobago                  19\n",
    " Laos                             18\n",
    " Thailand                         18\n",
    " Yugoslavia                       16\n",
    " Outlying-US(Guam-USVI-etc)       14\n",
    " Hungary                          13\n",
    " Honduras                         13\n",
    " Scotland                         12\n",
    " Holand-Netherlands                1\n",
    "Name: Native_country, dtype: int64\n",
    "====================================================================================================\n",
    "Income\n",
    " <=50K    24697\n",
    " >50K      7839\n",
    "Name: Income, dtype: int64\n",
    "===================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea94e73",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cdbec9",
   "metadata": {},
   "source": [
    "### Start EDA by analysing our target variable first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f9344",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('husl')\n",
    "f,ax=plt.subplots(1,2,figsize=(18,10))\n",
    "df['Income'].value_counts().plot.pie(explode=[0,0.1],autopct='%3.1f%%',\n",
    "                                          textprops ={ 'fontweight': 'bold','fontsize':18}, ax=ax[0],shadow=True)\n",
    "ax[0].set_title('Population Distribution', fontsize=22,fontweight ='bold')\n",
    "ax[0].set_ylabel('')\n",
    "sns.countplot('Income',data=df,ax=ax[1])\n",
    "ax[1].set_title('Income Distribution',fontsize=22,fontweight ='bold')\n",
    "ax[1].set_xlabel(\"Income\",fontsize=18,fontweight ='bold')\n",
    "plt.xticks(fontsize=18,fontweight ='bold')\n",
    "plt.show()\n",
    "\n",
    "Observation :\n",
    "75.9 % population (24697 peoples) have income less than 50K.\n",
    "Our task is to predict income and we see that target variable income is imbalanced.\n",
    "Let check each feature against Target variable to gain more insight into data before finding answer of important questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881bfd4",
   "metadata": {},
   "source": [
    "### Exploration of Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f9df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Minimum Age:', df['Age'].min(),'years')\n",
    "print('Maximum Age:', df['Age'].max(),'years')\n",
    "print('Average Age:', df['Age'].mean(),'years')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59416a32",
   "metadata": {},
   "source": [
    "Minimum Age: 17 years\n",
    "Maximum Age: 90 years\n",
    "Average Age: 38.58553602163757 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63aab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to create new columns to classify people according different age gruop\n",
    "def age_group(x):\n",
    "    x = int(x)\n",
    "    x = abs(x)\n",
    "    if (18 < x < 31):\n",
    "        return \"19-30\"\n",
    "    if (30 < x < 41) :\n",
    "        return '31-40'\n",
    "    if (40 < x <51):\n",
    "        return '41-50'\n",
    "    if (50 < x < 61):\n",
    "        return \"51-60\"\n",
    "    if (60 < x < 71):\n",
    "        return \"61-70\"\n",
    "    else :\n",
    "        return 'Greater Than 70'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bba144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling function age_group\n",
    "df['age_group']=df['Age'].apply(age_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cba8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "order_list = ['19-30','31-40','41-50','51-60','61-70','Greater Than 70']\n",
    "p = sns.countplot(df['age_group'], hue=df['Income'], palette='hsv', order=order_list)\n",
    "plt.title('Income of Individuals of Different Age Groups', fontsize=22, fontweight='bold')\n",
    "p.set_xlabel('Age Groups')\n",
    "plt.xticks(fontsize=16,fontweight ='bold')\n",
    "plt.yticks(fontsize=16,fontweight ='bold')\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc1f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of age with income categories\n",
    "ax= sns.FacetGrid(df,col='Income')\n",
    "ax.map(sns.distplot,'Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Observation:\n",
    "Here comes an interesting observation. We already know that only 24% people earnings more than 50K dollors and this plot tell us which age group then belong.\n",
    "\n",
    "almost 10 % people in age group of 19-30 earns more than 50 K dollars and this count drop even more in old peoples having age greater than 70. Might be some lucky old ones has created good Retirement Plan compare to rest old peoples.\n",
    "We can find maximum people earning more than 50K dollors belong to age group 41-50 & 51-60. It will be interesting to find out which profession lead this income to this category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8483e4",
   "metadata": {},
   "source": [
    "### So let dive into workclass and see what insight we get from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9dd611",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('rainbow')\n",
    "plt.figure(figsize=(10,10))\n",
    "df['Workclass'].value_counts().plot.pie(explode=[0,0.1,0.125,0.175,0.225,0.25,0.3,0.7],autopct='%2.1f%%',\n",
    "                                          textprops ={ 'fontsize':13}, shadow=True)\n",
    "plt.title('Population distribution as per Workclass', fontsize=20,fontweight ='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e0bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,9))\n",
    "p = sns.countplot(df['Workclass'], hue=df['Income'], palette='hsv')\n",
    "plt.title('Income distribution as per Workclass', fontsize=22, fontweight='bold')\n",
    "p.set_xlabel('Workclass',fontsize=18,fontweight ='bold')\n",
    "plt.xticks(fontsize=16,fontweight ='bold',rotation=30)\n",
    "plt.yticks(fontsize=16,fontweight ='bold')\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629917ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['Income'],df[\"Workclass\"], margins=True).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Workclass\tFederal-gov\tLocal-gov\tNever-worked\tPrivate\tSelf-emp-inc\tSelf-emp-not-inc\tState-gov\tWithout-pay\tAll\n",
    "Income\t\t\t\t\t\t\t\t\t\n",
    "<=50K\t589\t1476\t7\t19357\t494\t1816\t944\t14\t24697\n",
    ">50K\t371\t617\t0\t5152\t622\t724\t353\t0\t7839\n",
    "All\t960\t2093\t7\t24509\t1116\t2540\t1297\t14\t32536\n",
    "\n",
    "Observation :\n",
    "75.3% people belongs to Private sector followed by Self Emplyoed not Incorporated with 7.8 %\n",
    "\n",
    "Interseting observation comes with Self-Employed Incorporated category where the number of people who earn more than 50K dollars exceed those earning less than it !\n",
    "\n",
    "There is significant difference between private sector employees income where less than 25 % people earn more than 50K dollars an year.\n",
    "\n",
    "Another quite surprising insight comes from Federal Goverment countplot where We can see very minute difference between the number of people whose income is more or less than 50K dollars an year. Same goes with state goverment with some difference.\n",
    "\n",
    "Yes there exist category for who never worked and Working without pay but very few people belong to it.These people may be unempolyeed or colleges students. But Number is less And thats Good !!!\n",
    "\n",
    "Certainly Goverment Emplyoees have high wages compare to private !!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8b01b2",
   "metadata": {},
   "source": [
    "### Education Vs Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd76f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('rainbow')\n",
    "plt.figure(figsize=(10,10))\n",
    "df['Education'].value_counts().plot.pie(autopct='%2.1f%%', textprops ={ 'fontsize':13}, shadow=True)\n",
    "plt.title('Population distribution as per Education', fontsize=20,fontweight ='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7200249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,9))\n",
    "p = sns.countplot(df['Education'], hue=df['Income'], palette='hsv')\n",
    "plt.title('Income distribution as per Education', fontsize=22, fontweight='bold')\n",
    "p.set_xlabel('Education',fontsize=18,fontweight ='bold')\n",
    "plt.xticks(fontsize=16,fontweight ='bold',rotation=30)\n",
    "plt.yticks(fontsize=16,fontweight ='bold')\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b411c",
   "metadata": {},
   "source": [
    "Observation:\n",
    "Out of all population 32.3% people with HS-Grad education which is most prevalent education level.\n",
    "5.3 % people with masters and 1.3 % population is Doctorate degree holder. Highly educated & intellactual people belongs to this category.\n",
    "There are very handful people who earn more than 50 K dollars having education below 12th level.\n",
    "Maximum number of people who earn more than 50K dollars are Bachelors degree holder.\n",
    "Here comes interesting observation about income of highly educated people. In case of people belonging to Masters, Doctorate and Prof-School category in the education level, the number people earning more than 50K dollars an year outnumber than the number of people earning less than it.\n",
    "In case of Assoc-acad or Assoc-voc, there are a few people who earn more than 50K dollars an year!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe24a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['Education'],df[\"Workclass\"], margins=True).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68078e10",
   "metadata": {},
   "source": [
    "Workclass\tFederal-gov\tLocal-gov\tNever-worked\tPrivate\tSelf-emp-inc\tSelf-emp-not-inc\tState-gov\tWithout-pay\tAll\n",
    "Education\t\t\t\t\t\t\t\t\t\n",
    "10th\t6\t31\t2\t795\t19\t67\t13\t0\t933\n",
    "11th\t9\t36\t1\t1041\t14\t60\t14\t0\t1175\n",
    "12th\t5\t19\t0\t373\t7\t19\t10\t0\t433\n",
    "1st-4th\t0\t4\t0\t146\t2\t13\t1\t0\t166\n",
    "5th-6th\t1\t9\t0\t295\t4\t19\t4\t0\t332\n",
    "7th-8th\t2\t28\t1\t495\t14\t94\t10\t1\t645\n",
    "9th\t3\t23\t0\t438\t10\t34\t6\t0\t514\n",
    "Assoc-acdm\t55\t88\t0\t776\t35\t71\t41\t1\t1067\n",
    "Assoc-voc\t38\t86\t0\t1066\t38\t108\t46\t0\t1382\n",
    "Bachelors\t212\t477\t0\t3722\t273\t399\t269\t0\t5352\n",
    "Doctorate\t16\t27\t0\t196\t35\t50\t89\t0\t413\n",
    "HS-grad\t263\t503\t1\t8305\t279\t866\t268\t9\t10494\n",
    "Masters\t67\t342\t0\t941\t79\t124\t169\t0\t1722\n",
    "Preschool\t0\t4\t0\t45\t0\t0\t1\t0\t50\n",
    "Prof-school\t29\t29\t0\t275\t81\t131\t31\t0\t576\n",
    "Some-college\t254\t387\t2\t5600\t226\t485\t325\t3\t7282\n",
    "All\t960\t2093\t7\t24509\t1116\t2540\t1297\t14\t32536\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee002767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage Income distribution in terms of Education\n",
    "sns.set_palette('rainbow')\n",
    "table = pd.crosstab(df['Education'], df['Income'])\n",
    "(table.div(table.sum(axis=1),axis=0)*100).plot(kind='bar',stacked=True,figsize=(15,7))\n",
    "plt.title('Percent Income distribution as per Education', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Education', fontsize=18,fontweight='bold')\n",
    "plt.ylabel('Population', fontsize=18,fontweight='bold')\n",
    "plt.xticks(fontweight ='bold')\n",
    "plt.show("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4adb3f0",
   "metadata": {},
   "source": [
    "Observation:\n",
    "In terms of percentage 75 % of Doctorate people earn more than 50K dollars an years, followed by Prof- school.\n",
    "So, Highly earning people most of times comes with Masters or Doctorate education background.\n",
    "From crosstab we can see that private sector is highest recuriter for HS-Grad with 8305 people.\n",
    "Private sector is also major recurtier of highly eduacated people with masters, doctorate followed by local goverment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e2b86",
   "metadata": {},
   "source": [
    "### Impact of Marital Status on Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('Set1')\n",
    "plt.figure(figsize=(10,10))\n",
    "df['Marital_status'].value_counts().plot.pie(autopct='%2.1f%%', explode=[0.075,0.1,0.125,0.15,0.175,0.2,0.225],\n",
    "                                             textprops ={'fontsize':13,'fontweight':'bold'},shadow=True)\n",
    "plt.title('Population distribution as per Marital Status', fontsize=20,fontweight ='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7963d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage Income distribution in terms of Education\n",
    "sns.set_palette('rainbow')\n",
    "table = pd.crosstab(df['Marital_status'], df['Income'])\n",
    "(table.div(table.sum(axis=1),axis=0)*100).plot(kind='bar',stacked=True,figsize=(12,8))\n",
    "plt.title('Percent Income distribution as per Marital Status', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Marital Status', fontsize=18,fontweight='bold')\n",
    "plt.ylabel('Population', fontsize=18,fontweight='bold')\n",
    "plt.xticks(fontweight ='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['Marital_status'],df[\"Income\"], margins=True).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b570c0",
   "metadata": {},
   "source": [
    "Marital_status\tDivorced\tMarried-AF-spouse\tMarried-civ-spouse\tMarried-spouse-absent\tNever-married\tSeparated\tWidowed\tAll\n",
    "Income\t\t\t\t\t\t\t\t\n",
    "<=50K\t3978\t13\t8280\t384\t10175\t959\t908\t24697\n",
    ">50K\t463\t10\t6690\t34\t491\t66\t85\t7839\n",
    "All\t4441\t23\t14970\t418\t10666\t1025\t993\t32536\n",
    "Observation :\n",
    "Married people are most like to earn more than 50K dollars an year.\n",
    "\n",
    "Reason Might be Emotional support and stability of life than who are divorced/separated.\n",
    "\n",
    "One thing to note here is that Married-civ-spouse is the only category which has comparable number of people belonging to both categories.\n",
    "For others, there are less than 25% of the adults earning more than 50K dollars an year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4956c2",
   "metadata": {},
   "source": [
    "### Occupation VS Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b8609",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('rainbow')\n",
    "plt.figure(figsize=(10,10))\n",
    "df['Occupation'].value_counts().plot.pie(autopct='%2.1f%%', textprops ={ 'fontsize':13}, shadow=True)\n",
    "plt.title('Population distribution as per Occupation', fontsize=20,fontweight ='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a16696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,9))\n",
    "p = sns.countplot(df['Occupation'], hue=df['Income'], palette='cool')\n",
    "plt.title('Income distribution as per Occupation', fontsize=22, fontweight='bold')\n",
    "p.set_xlabel('Occupation',fontsize=18,fontweight ='bold')\n",
    "plt.xticks(fontsize=16,rotation=90)\n",
    "plt.yticks(fontsize=16,fontweight ='bold')\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "Observation :\n",
    "Exec-managerial role are equally likely to earn more than 50K dollars an year.\n",
    "Peoples working in Arm- Forces, Farming-fishing, Machine-op-inspect, Other-service, Adm-clerical, Handlers-cleaners are very less likely to earn more than 50K dollars an year.\n",
    "Around 25% of the people working in Sales earn more than 50K dollars an year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e4e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage Income distribution in terms of Education\n",
    "sns.set_palette('rainbow')\n",
    "table = pd.crosstab(df['Occupation'], df['Income'])\n",
    "(table.div(table.sum(axis=1),axis=0)*100).plot(kind='bar',stacked=True,figsize=(12,8))\n",
    "plt.title('Percent Income distribution as per Occupation', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Occupation', fontsize=18,fontweight='bold')\n",
    "plt.ylabel('Population', fontsize=18,fontweight='bold')\n",
    "plt.xticks(fontweight ='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68909e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['Occupation'],df[\"Income\"], margins=True).style.background_gradient(cmap='winter_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Income\t<=50K\t>50K\tAll\n",
    "Occupation\t\t\t\n",
    "Adm-clerical\t3260\t507\t3767\n",
    "Armed-Forces\t8\t1\t9\n",
    "Craft-repair\t3165\t929\t4094\n",
    "Exec-managerial\t2097\t1968\t4065\n",
    "Farming-fishing\t877\t115\t992\n",
    "Handlers-cleaners\t1283\t86\t1369\n",
    "Machine-op-inspct\t1751\t249\t2000\n",
    "Other-service\t3154\t137\t3291\n",
    "Priv-house-serv\t146\t1\t147\n",
    "Prof-specialty\t3930\t2049\t5979\n",
    "Protective-serv\t438\t211\t649\n",
    "Sales\t2667\t983\t3650\n",
    "Tech-support\t644\t283\t927\n",
    "Transport-moving\t1277\t320\t1597\n",
    "All\t24697\t7839\t32536\n",
    "Observation :\n",
    "Prof-Speciality position with maximum 2049 peoples followed by Exec-managerial position with 1968 people leads chart for greater than 50K dollars an year.\n",
    "If we talk in term of percentage probablity Exective Managerial position have 50% chances to lead in Greater than 50K dollars Club.\n",
    "There's close to 33% probablity for an adult in Prof-specialty to earn more than 50K dollars an year.\n",
    "- Minimum chances for Greater than 50K Club comes from Private House Service occupation where 1/147 chance of getting more than 50K dollars an year.\n",
    "\n",
    "Let check where these different occupation fit with respect to Work class through crosstab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['Occupation'],df[\"Workclass\"], margins=True).style.background_gradient(cmap='winter_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23e8216",
   "metadata": {},
   "outputs": [],
   "source": [
    "Workclass\tFederal-gov\tLocal-gov\tNever-worked\tPrivate\tSelf-emp-inc\tSelf-emp-not-inc\tState-gov\tWithout-pay\tAll\n",
    "Occupation\t\t\t\t\t\t\t\t\t\n",
    "Adm-clerical\t317\t283\t0\t2831\t31\t50\t252\t3\t3767\n",
    "Armed-Forces\t9\t0\t0\t0\t0\t0\t0\t0\t9\n",
    "Craft-repair\t64\t146\t0\t3191\t106\t530\t56\t1\t4094\n",
    "Exec-managerial\t180\t214\t0\t2690\t400\t392\t189\t0\t4065\n",
    "Farming-fishing\t8\t29\t0\t453\t51\t430\t15\t6\t992\n",
    "Handlers-cleaners\t23\t47\t0\t1272\t2\t15\t9\t1\t1369\n",
    "Machine-op-inspct\t14\t12\t0\t1911\t13\t36\t13\t1\t2000\n",
    "Other-service\t35\t193\t0\t2736\t27\t175\t124\t1\t3291\n",
    "Priv-house-serv\t0\t0\t0\t147\t0\t0\t0\t0\t147\n",
    "Prof-specialty\t175\t705\t7\t4145\t160\t373\t414\t0\t5979\n",
    "Protective-serv\t28\t304\t0\t190\t5\t6\t116\t0\t649\n",
    "Sales\t14\t7\t0\t2942\t291\t385\t11\t0\t3650\n",
    "Tech-support\t68\t38\t0\t735\t3\t26\t57\t0\t927\n",
    "Transport-moving\t25\t115\t0\t1266\t27\t122\t41\t1\t1597\n",
    "All\t960\t2093\t7\t24509\t1116\t2540\t1297\t14\t32536\n",
    "Observation :\n",
    "Here is another surprising element, there are 7 peoples From Never Worked Category with High earning Profession of Prof-speciality. Same from Without pay workclass we find 6 peoples with Exec-Managerial. Possibly they are working with unpaid internship profile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c17ef4a",
   "metadata": {},
   "source": [
    "### Relationship VS Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ea7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('rainbow')\n",
    "plt.figure(figsize=(10,10))\n",
    "df['Relationship'].value_counts().plot.pie(autopct='%2.1f%%',explode=[0.05,0.05,0.05,0.05,0.075,0.075],\n",
    "                                           textprops ={'fontweight' :'bold', 'fontsize':13}, shadow=True)\n",
    "plt.title('Population distribution as per Relationship', fontsize=22,fontweight ='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2fcfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "p = sns.countplot(df['Relationship'], hue=df['Income'], palette='cool')\n",
    "plt.title('Income distribution as per Occupation', fontsize=22, fontweight='bold')\n",
    "p.set_xlabel('Relationship',fontsize=18,fontweight ='bold')\n",
    "plt.xticks(fontsize=16,rotation=20)\n",
    "plt.yticks(fontsize=16,fontweight ='bold')\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e2d6e",
   "metadata": {},
   "source": [
    "Observation:\n",
    "Wives are equally likely to earn more than 50K dollars an year.\n",
    "For Husbands, although significant, there is less possibility of them to earn more than 50K dollars an year.\n",
    "There are just a handful of Unmarried people earning more than 50K dollars an year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ac348",
   "metadata": {},
   "source": [
    "### Race vs Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('gist_rainbow_r')\n",
    "plt.figure(figsize=(10,10))\n",
    "df['Race'].value_counts().plot.pie(autopct='%2.1f%%',explode=[0.05,0.05,0.05,0.05,0.075],\n",
    "                                           textprops ={'fontweight' :'bold', 'fontsize':13}, shadow=True)\n",
    "plt.title('Race distribution as per Relationship', fontsize=22,fontweight ='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36c387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "p = sns.countplot(df['Race'], hue=df['Income'], palette='cool')\n",
    "plt.title('Race distribution as per Occupation', fontsize=22, fontweight='bold')\n",
    "p.set_xlabel('Race',fontsize=18,fontweight ='bold')\n",
    "plt.xticks(fontsize=16,rotation=20)\n",
    "plt.yticks(fontsize=16,fontweight ='bold')\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98590b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage Income distribution in terms of Education\n",
    "sns.set_palette('viridis')\n",
    "table = pd.crosstab(df['Race'], df['Income'])\n",
    "(table.div(table.sum(axis=1),axis=0)*100).plot(kind='bar',stacked=True,figsize=(12,8))\n",
    "plt.title('Percent Income distribution as per Race', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Race', fontsize=18,fontweight='bold')\n",
    "plt.ylabel('Population', fontsize=18,fontweight='bold')\n",
    "plt.xticks(fontweight ='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6c12c3",
   "metadata": {},
   "source": [
    "Observation:\n",
    "85.4 % peoples are whites followed by black community with 9.6%.\n",
    "Maximum peoples in Greater than 50K Club are White.\n",
    " This observation is biased as we have very less data of other races. So Nothing Meaningful & reliable insight we can draw from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a0d47",
   "metadata": {},
   "source": [
    "### Sex Vs Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba722eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('plasma')\n",
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "df['Sex'].value_counts().plot.pie(explode=[0,0.1],autopct='%3.1f%%',\n",
    "                                          textprops ={ 'fontweight': 'bold','fontsize':18}, ax=ax[0],shadow=True)\n",
    "ax[0].set_title('Population Distribution', fontsize=22,fontweight ='bold')\n",
    "ax[0].set_ylabel('')\n",
    "sns.countplot('Sex',hue=df['Income'] ,data=df,ax=ax[1])\n",
    "ax[1].set_title('Income Distribution',fontsize=22,fontweight ='bold')\n",
    "ax[1].set_xlabel(\"Income\",fontsize=18,fontweight ='bold')\n",
    "plt.xticks(fontsize=18,fontweight ='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376605e0",
   "metadata": {},
   "source": [
    "Observation:\n",
    "Significant gap between male and female earnings.\n",
    "Less than 10% Women in Greater Than 50K Dollar Club.\n",
    "Around 33% of Men earning more than 50K dollars an year.\n",
    "Let find in which profession these women involved in ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a0efd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['Occupation'],[df.Sex,df.Income], margins=True).style.background_gradient(cmap='winter_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee9d8ef",
   "metadata": {},
   "source": [
    "Sex\tFemale\tMale\tAll\n",
    "Income\t<=50K\t>50K\t<=50K\t>50K\t\n",
    "Occupation\t\t\t\t\t\n",
    "Adm-clerical\t2323\t212\t937\t295\t3767\n",
    "Armed-Forces\t0\t0\t8\t1\t9\n",
    "Craft-repair\t202\t20\t2963\t909\t4094\n",
    "Exec-managerial\t879\t280\t1218\t1688\t4065\n",
    "Farming-fishing\t63\t2\t814\t113\t992\n",
    "Handlers-cleaners\t160\t4\t1123\t82\t1369\n",
    "Machine-op-inspct\t529\t20\t1222\t229\t2000\n",
    "Other-service\t1748\t51\t1406\t86\t3291\n",
    "Priv-house-serv\t138\t1\t8\t0\t147\n",
    "Prof-specialty\t1917\t437\t2013\t1612\t5979\n",
    "Protective-serv\t66\t10\t372\t201\t649\n",
    "Sales\t1175\t88\t1492\t895\t3650\n",
    "Tech-support\t302\t45\t342\t238\t927\n",
    "Transport-moving\t81\t9\t1196\t311\t1597\n",
    "All\t9583\t1179\t15114\t6660\t32536\n",
    "\n",
    "Observation:\n",
    "Out of all population 33 % are womens. Inside that most of females work in Adm-clerical,Other-service,Prof-specialty.\n",
    "Same as in case of Men Maximum number in Greater than 50K dollars Club in females comes from Prof-speciality,Exec-Managerial profession.\n",
    "Maximum men work in Prof-Specialty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf3b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Parameters\n",
    "plt.rcParams['figure.figsize'] = [15,15]\n",
    "sns.set(style = 'darkgrid')\n",
    "\n",
    "# This Violin plot show how capital gain, loss, hours per week and education vary with the race of the people\n",
    "plt.subplot(2,2,1)\n",
    "sns.violinplot(x = df['Race'], y = df['Capital_gain'], data = df);\n",
    "plt.subplot(2,2,2)\n",
    "sns.violinplot(x = df['Race'], y = df['Capital_loss'], data = df);\n",
    "plt.subplot(2,2,3)\n",
    "sns.violinplot(x = df['Race'], y = df['Hours_per_week'], data = df);\n",
    "plt.subplot(2,2,4)\n",
    "sns.violinplot(x = df['Race'], y = df['Education_num'], data = df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2882708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the hours per week according to the education of the person\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns_grad = sns.barplot(x = df['Education'], y = df['Hours_per_week'], data = df)\n",
    "plt.setp(sns_grad.get_xticklabels(), rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb3e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the pairwise relation in the dataset.\n",
    "sns.pairplot(df,hue=\"Income\",palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af9b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af99d4",
   "metadata": {},
   "source": [
    "\tAge\tWorkclass\tFnlwgt\tEducation\tEducation_num\tMarital_status\tOccupation\tRelationship\tRace\tSex\tCapital_gain\tCapital_loss\tHours_per_week\tNative_country\tIncome\n",
    "0\t50\tSelf-emp-not-inc\t83311\tBachelors\t13\tMarried-civ-spouse\tExec-managerial\tHusband\tWhite\tMale\t0\t0\t13\tUnited-States\t<=50K\n",
    "1\t38\tPrivate\t215646\tHS-grad\t9\tDivorced\tHandlers-cleaners\tNot-in-family\tWhite\tMale\t0\t0\t40\tUnited-States\t<=50K\n",
    "2\t53\tPrivate\t234721\t11th\t7\tMarried-civ-spouse\tHandlers-cleaners\tHusband\tBlack\tMale\t0\t0\t40\tUnited-States\t<=50K\n",
    "3\t28\tPrivate\t338409\tBachelors\t13\tMarried-civ-spouse\tProf-specialty\tWife\tBlack\tFemale\t0\t0\t40\tCuba\t<=50K\n",
    "4\t37\tPrivate\t284582\tMasters\t14\tMarried-civ-spouse\tExec-managerial\tWife\tWhite\tFemale\t0\t0\t40\tUnited-States\t<=50K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09880316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\tAge\tWorkclass\tFnlwgt\tEducation\tEducation_num\tMarital_status\tOccupation\tRelationship\tRace\tSex\tCapital_gain\tCapital_loss\tHours_per_week\tNative_country\tIncome\n",
    "0\t50\tSelf-emp-not-inc\t83311\tBachelors\t13\tMarried-civ-spouse\tExec-managerial\tHusband\tWhite\tMale\t0\t0\t13\tUnited-States\t<=50K\n",
    "1\t38\tPrivate\t215646\tHS-grad\t9\tDivorced\tHandlers-cleaners\tNot-in-family\tWhite\tMale\t0\t0\t40\tUnited-States\t<=50K\n",
    "2\t53\tPrivate\t234721\t11th\t7\tMarried-civ-spouse\tHandlers-cleaners\tHusband\tBlack\tMale\t0\t0\t40\tUnited-States\t<=50K\n",
    "3\t28\tPrivate\t338409\tBachelors\t13\tMarried-civ-spouse\tProf-specialty\tWife\tBlack\tFemale\t0\t0\t40\tCuba\t<=50K\n",
    "4\t37\tPrivate\t284582\tMasters\t14\tMarried-civ-spouse\tExec-managerial\tWife\tWhite\tFemale\t0\t0\t40\tUnited-States\t<=50K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0ac80f",
   "metadata": {},
   "source": [
    "### Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Label Encoder on categorical variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "for i in Category:\n",
    "    df[i] = le.fit_transform(df[i])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9bf0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Label Encoder on categorical variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "for i in Category:\n",
    "    df[i] = le.fit_transform(df[i])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab63cdf",
   "metadata": {},
   "source": [
    "\tAge\tWorkclass\tFnlwgt\tEducation\tEducation_num\tMarital_status\tOccupation\tRelationship\tRace\tSex\tCapital_gain\tCapital_loss\tHours_per_week\tNative_country\tIncome\n",
    "0\t50\t5\t83311\t9\t13\t2\t3\t0\t4\t1\t0\t0\t13\t38\t0\n",
    "1\t38\t3\t215646\t11\t9\t0\t5\t1\t4\t1\t0\t0\t40\t38\t0\n",
    "2\t53\t3\t234721\t1\t7\t2\t5\t0\t2\t1\t0\t0\t40\t38\t0\n",
    "3\t28\t3\t338409\t9\t13\t2\t9\t5\t2\t0\t0\t0\t40\t4\t0\n",
    "4\t37\t3\t284582\t12\t14\t2\t3\t5\t4\t0\t0\t0\t40\t38\t0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58604e",
   "metadata": {},
   "source": [
    "## Feature selection and Engineering\n",
    "1. Outliers Detection and Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a406ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10),facecolor='white')\n",
    "plotnumber=1\n",
    "\n",
    "for column in Numerical:\n",
    "    if plotnumber<=6:\n",
    "        ax=plt.subplot(2,3,plotnumber)\n",
    "        sns.boxplot(df[column],color='g')\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f08cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping unnecessary columns\n",
    "df.drop([\"Fnlwgt\", \"Education\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4979c2",
   "metadata": {},
   "source": [
    "## Outliers removal using Zscore method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "z = np.abs(zscore(df))\n",
    "threshold = 3\n",
    "df1 = df[(z<3).all(axis = 1)]\n",
    "\n",
    "print (\"Shape of the dataframe before removing outliers: \", df.shape)\n",
    "print (\"Shape of the dataframe after removing outliers: \", df1.shape)\n",
    "print (\"Percentage of data loss post outlier removal: \", (df.shape[0]-df1.shape[0])/df.shape[0]*100)\n",
    "\n",
    "df=df1.copy() # reassigning the changed dataframe name to our original dataframe name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61785168",
   "metadata": {},
   "source": [
    "Shape of the dataframe before removing outliers:  (32536, 13)\n",
    "Shape of the dataframe after removing outliers:  (28061, 13)\n",
    "Percentage of data loss post outlier removal:  13.753995574133269\n",
    "\n",
    "Data Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4cf0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Percentage Data Loss :'+\"\\033[0m\",((32536-28061)/32536)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e63f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "Percentage Data Loss : 13.753995574133269 %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525bb660",
   "metadata": {},
   "source": [
    "### We are losing 13.75 % of data. Its big but we can afford it. Considering we have a lot of rows in our datatset for ML model building. We have option to go for quantile method but by looking at boxplot we can say lower capping will result in useful data loss and eventually data loss will be more than zscore method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d977ecc",
   "metadata": {},
   "source": [
    "### 2. Skewness of features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed92b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22,5),facecolor='white')\n",
    "plotnum=1\n",
    "for col in df[['Age','Capital_gain','Capital_loss','Hours_per_week']]:\n",
    "    if plotnum<=4:\n",
    "        plt.subplot(1,4,plotnum)\n",
    "        sns.distplot(df[col],color='r')\n",
    "        plt.xlabel(col,fontsize=20)\n",
    "    plotnum+=1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba81768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829ef80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Age                0.487918\n",
    "Workclass          0.080746\n",
    "Education_num     -0.147601\n",
    "Marital_status    -0.046623\n",
    "Occupation         0.015247\n",
    "Relationship       0.752334\n",
    "Race              -2.549199\n",
    "Sex               -0.689977\n",
    "Capital_gain       4.924729\n",
    "Capital_loss      29.669292\n",
    "Hours_per_week    -0.359365\n",
    "Native_country    -5.348195\n",
    "Income             1.322011\n",
    "dtype: float64\n",
    "Observation :\n",
    "Relationship,Sex,Native_country,Income are skewed but as they are categorical concept of skewness doesnot mean anything to it.\n",
    "Capital_gain and Capital_loss are numeric variable with lot of zero and high number. So skewness exist in them. There is no point in transforming it because at end data will be skewed.\n",
    "We will have option of scaling data to handle skewness in Capital gain and Capital loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0fc417",
   "metadata": {},
   "source": [
    "### 3. Corrleation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a4368",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Age\tWorkclass\tEducation_num\tMarital_status\tOccupation\tRelationship\tRace\tSex\tCapital_gain\tCapital_loss\tHours_per_week\tNative_country\tIncome\n",
    "Age\t1.000000\t0.029674\t0.045622\t-0.286328\t-0.001610\t-0.270591\t0.021292\t0.089469\t0.131998\t0.023459\t0.090783\t0.037494\t0.241844\n",
    "Workclass\t0.029674\t1.000000\t-0.002200\t-0.015623\t0.008779\t-0.057823\t0.067532\t0.069617\t0.011083\t-0.010613\t0.028104\t0.001938\t-0.007699\n",
    "Education_num\t0.045622\t-0.002200\t1.000000\t-0.058587\t0.073142\t-0.092447\t0.080766\t0.003688\t0.157858\t0.007348\t0.156303\t0.149688\t0.320271\n",
    "Marital_status\t-0.286328\t-0.015623\t-0.058587\t1.000000\t0.036477\t0.183248\t-0.084558\t-0.125361\t-0.069601\t-0.016396\t-0.196053\t-0.032575\t-0.194146\n",
    "Occupation\t-0.001610\t0.008779\t0.073142\t0.036477\t1.000000\t-0.038735\t0.000613\t0.051055\t0.009446\t-0.004779\t-0.022194\t0.018069\t0.030664\n",
    "Relationship\t-0.270591\t-0.057823\t-0.092447\t0.183248\t-0.038735\t1.000000\t-0.146432\t-0.577195\t-0.089095\t0.030105\t-0.261919\t-0.011274\t-0.248263\n",
    "Race\t0.021292\t0.067532\t0.080766\t-0.084558\t0.000613\t-0.146432\t1.000000\t0.122731\t0.031333\t0.000186\t0.055486\t0.018660\t0.094756\n",
    "Sex\t0.089469\t0.069617\t0.003688\t-0.125361\t0.051055\t-0.577195\t0.122731\t1.000000\t0.069539\t-0.027323\t0.236168\t-0.011189\t0.211792\n",
    "Capital_gain\t0.131998\t0.011083\t0.157858\t-0.069601\t0.009446\t-0.089095\t0.031333\t0.069539\t1.000000\t-0.009077\t0.098705\t0.019185\t0.343487\n",
    "Capital_loss\t0.023459\t-0.010613\t0.007348\t-0.016396\t-0.004779\t0.030105\t0.000186\t-0.027323\t-0.009077\t1.000000\t-0.003440\t0.002925\t-0.016465\n",
    "Hours_per_week\t0.090783\t0.028104\t0.156303\t-0.196053\t-0.022194\t-0.261919\t0.055486\t0.236168\t0.098705\t-0.003440\t1.000000\t0.004380\t0.233472\n",
    "Native_country\t0.037494\t0.001938\t0.149688\t-0.032575\t0.018069\t-0.011274\t0.018660\t-0.011189\t0.019185\t0.002925\t0.004380\t1.000000\t0.047240\n",
    "Income\t0.241844\t-0.007699\t0.320271\t-0.194146\t0.030664\t-0.248263\t0.094756\t0.211792\t0.343487\t-0.016465\t0.233472\t0.047240\t1.000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d067cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,15))\n",
    "sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True, square=True, fmt='0.3f', \n",
    "            annot_kws={'size':10}, cmap=\"gist_stern\")\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f61edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,6))\n",
    "df.corr()['Income'].drop(['Income']).sort_values(ascending=False).plot(kind='bar',color = 'purple')\n",
    "plt.xlabel('Features',fontsize=15)\n",
    "plt.ylabel('Income',fontsize=15)\n",
    "plt.title('Correlation of features with Target Variable Income',fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8fd7cd",
   "metadata": {},
   "source": [
    "Observation:\n",
    "Capital loss and workclass,occupation, native country, race are correlated with target variable with less than 10% correlation. After checking Mulitcollinearity we will decide to drop these poorly correlated features or go for PCA.\n",
    "\n",
    "As high or low Correlation doesnot mean its causation !!!\n",
    "\n",
    "Between input features maximum correlation of -0.557 exist between sex and relationship.\n",
    "\n",
    "Capital gain is highly correlated with target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a665d",
   "metadata": {},
   "source": [
    "## 4. Checking Multicollinearity between features using variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b70d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif= pd.DataFrame()\n",
    "vif['VIF']= [variance_inflation_factor(df.values,i) for i in range(df.shape[1])]\n",
    "vif['Features']= df.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a273d184",
   "metadata": {},
   "source": [
    "VIF\tFeatures\n",
    "0\t10.452897\tAge\n",
    "1\t8.807425\tWorkclass\n",
    "2\t21.287850\tEducation_num\n",
    "3\t4.358505\tMarital_status\n",
    "4\t3.357557\tOccupation\n",
    "5\t2.937270\tRelationship\n",
    "6\t36.291318\tRace\n",
    "7\t4.566872\tSex\n",
    "8\t1.209589\tCapital_gain\n",
    "9\t1.004295\tCapital_loss\n",
    "10\t15.202917\tHours_per_week\n",
    "11\t79.077624\tNative_country\n",
    "12\t1.777448\tIncome\n",
    "Strategy to Address Multicollinearity :\n",
    "Removing Some of highly correlated features. But this will not work here as most of input features are correlated with each other either moderated or poorly.\n",
    "Another way to address Multicollinerity is to Scaled Data and then apply PCA.\n",
    "We will go by Second way for further investigation. As For some Independent feature VIF is exceed permissible limit of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f76437",
   "metadata": {},
   "source": [
    "## 5. Balanceing Imbalanced target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f45705",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Income.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddaa776",
   "metadata": {},
   "source": [
    "0    21767\n",
    "1     6294\n",
    "Name: Income, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6936c8f9",
   "metadata": {},
   "source": [
    "As Target variable data is Imbalanced in nature we will need to balance target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdeb59a",
   "metadata": {},
   "source": [
    "### Balancing using SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f88134",
   "metadata": {},
   "source": [
    "### As data is Imbalanced in nature we will need to balance target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325aa36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampleing using SMOTE Techniques\n",
    "oversample = SMOTE()\n",
    "X, Y = oversample.fit_resample(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab74c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85820747",
   "metadata": {},
   "source": [
    "1    1158\n",
    "0    1158\n",
    "Name: Attrition, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c03057",
   "metadata": {},
   "source": [
    "### Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe204de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler= StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd935d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "#plot the graph to find the principal components\n",
    "x_pca = pca.fit_transform(X_scale)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), 'ro-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance %')\n",
    "plt.title('Explained variance Ratio')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a876f",
   "metadata": {},
   "source": [
    "Comment -\n",
    "AS per the graph, we can see that 21 principal components attribute for 90% of variation in the data. We shall pick the first 21 components for our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_new = PCA(n_components=21)\n",
    "x_new = pca_new.fit_transform(X_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72150abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "principle_x=pd.DataFrame(x_new,columns=np.arange(21))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc8654f",
   "metadata": {},
   "source": [
    "### Machine Learning Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b82fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd813244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a21c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(principle_x, Y, random_state=42, test_size=.33)\n",
    "print('Training feature matrix size:',X_train.shape)\n",
    "print('Training target vector size:',Y_train.shape)\n",
    "print('Test feature matrix size:',X_test.shape)\n",
    "print('Test target vector size:',Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0a1be7",
   "metadata": {},
   "source": [
    "Training feature matrix size: (1551, 21)\n",
    "Training target vector size: (1551,)\n",
    "Test feature matrix size: (765, 21)\n",
    "Test target vector size: (765,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a15e18",
   "metadata": {},
   "source": [
    "### Finding best Random state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83341040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score\n",
    "maxAccu=0\n",
    "maxRS=0\n",
    "for i in range(1,250):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(principle_x,Y,test_size = 0.33, random_state=i)\n",
    "    log_reg=LogisticRegression()\n",
    "    log_reg.fit(X_train,Y_train)\n",
    "    y_pred=log_reg.predict(X_test)\n",
    "    acc=accuracy_score(Y_test,y_pred)\n",
    "    if acc>maxAccu:\n",
    "        maxAccu=acc\n",
    "        maxRS=i\n",
    "print('Best accuracy is', maxAccu ,'on Random_state', maxRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf8518",
   "metadata": {},
   "source": [
    "Best accuracy is 0.8705882352941177 on Random_state 242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfd2b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(principle_x, Y, random_state=242, test_size=.33)\n",
    "log_reg=LogisticRegression()\n",
    "log_reg.fit(X_train,Y_train)\n",
    "y_pred=log_reg.predict(X_test)\n",
    "print('\\033[1m'+'Logistics Regression Evaluation'+'\\033[0m')\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Accuracy Score of Logistics Regression :'+'\\033[0m', accuracy_score(Y_test, y_pred))\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Confusion matrix of Logistics Regression :'+'\\033[0m \\n',confusion_matrix(Y_test, y_pred))\n",
    "print('\\n')\n",
    "print('\\033[1m'+'classification Report of Logistics Regression'+'\\033[0m \\n',classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ce9c5c",
   "metadata": {},
   "source": [
    "Logistics Regression Evaluation\n",
    "\n",
    "\n",
    "Accuracy Score of Logistics Regression : 0.8705882352941177\n",
    "\n",
    "\n",
    "Confusion matrix of Logistics Regression : \n",
    " [[324  44]\n",
    " [ 55 342]]\n",
    "\n",
    "\n",
    "classification Report of Logistics Regression \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.85      0.88      0.87       368\n",
    "           1       0.89      0.86      0.87       397\n",
    "\n",
    "    accuracy                           0.87       765\n",
    "   macro avg       0.87      0.87      0.87       765\n",
    "weighted avg       0.87      0.87      0.87       765\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd86ca",
   "metadata": {},
   "source": [
    "## Finding Optimal value of n_neighbors for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd515e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse_val = [] #to store rmse values for different k\n",
    "for K in range(30):\n",
    "    K = K+1\n",
    "    model = neighbors.KNeighborsClassifier(n_neighbors = K)\n",
    "\n",
    "    model.fit(X_train,Y_train)  #fit the model\n",
    "    y_pred=model.predict(X_test) #make prediction on test set\n",
    "    error = sqrt(mean_squared_error(Y_test,y_pred)) #calculate rmse\n",
    "    rmse_val.append(error) #store rmse values\n",
    "    print('RMSE value for k= ' , K , 'is:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13767d0",
   "metadata": {},
   "source": [
    "RMSE value for k=  1 is: 0.3273981406234008\n",
    "RMSE value for k=  2 is: 0.30032661958503204\n",
    "RMSE value for k=  3 is: 0.34678730932445717\n",
    "RMSE value for k=  4 is: 0.31931298801289854\n",
    "RMSE value for k=  5 is: 0.35791699479543954\n",
    "RMSE value for k=  6 is: 0.34866692910423897\n",
    "RMSE value for k=  7 is: 0.3704792868174742\n",
    "RMSE value for k=  8 is: 0.3615507630310936\n",
    "RMSE value for k=  9 is: 0.37399101733297235\n",
    "RMSE value for k=  10 is: 0.3633540199183844\n",
    "RMSE value for k=  11 is: 0.3860305788964616\n",
    "RMSE value for k=  12 is: 0.37399101733297235\n",
    "RMSE value for k=  13 is: 0.39107694443752145\n",
    "RMSE value for k=  14 is: 0.3894020890135344\n",
    "RMSE value for k=  15 is: 0.39934587037179503\n",
    "RMSE value for k=  16 is: 0.39107694443752145\n",
    "RMSE value for k=  17 is: 0.4009791936316524\n",
    "RMSE value for k=  18 is: 0.3894020890135344\n",
    "RMSE value for k=  19 is: 0.3960590171906697\n",
    "RMSE value for k=  20 is: 0.39770583933420295\n",
    "RMSE value for k=  21 is: 0.40422604172722165\n",
    "RMSE value for k=  22 is: 0.40260589075170505\n",
    "RMSE value for k=  23 is: 0.4090479940519309\n",
    "RMSE value for k=  24 is: 0.4090479940519309\n",
    "RMSE value for k=  25 is: 0.41381376253739977\n",
    "RMSE value for k=  26 is: 0.40744701728620475\n",
    "RMSE value for k=  27 is: 0.41852526649272975\n",
    "RMSE value for k=  28 is: 0.4058397249567139\n",
    "RMSE value for k=  29 is: 0.41381376253739977\n",
    "RMSE value for k=  30 is: 0.4058397249567139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83777a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the rmse values against k values -\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(range(30), rmse_val, color='blue', linestyle='dashed', marker='o', markerfacecolor='green', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce5027",
   "metadata": {},
   "source": [
    "# Applying other classification algorith "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede659fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=[\n",
    "        SVC(),\n",
    "        GaussianNB(),\n",
    "        DecisionTreeClassifier(),\n",
    "        KNeighborsClassifier(n_neighbors = 22),\n",
    "        RandomForestClassifier(),\n",
    "        AdaBoostClassifier(),\n",
    "        GradientBoostingClassifier(),\n",
    "        BaggingClassifier()]\n",
    "\n",
    "for m in model:\n",
    "    m.fit(X_train,Y_train)\n",
    "    y_pred=m.predict(X_test)\n",
    "    print('\\033[1m'+'Classification ML Algorithm Evaluation Matrix',m,'is' +'\\033[0m')\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+'Accuracy Score :'+'\\033[0m\\n', accuracy_score(Y_test, y_pred))\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+'Confusion matrix :'+'\\033[0m \\n',confusion_matrix(Y_test, y_pred))\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+'Classification Report :'+'\\033[0m \\n',classification_report(Y_test, y_pred))\n",
    "    print('\\n')\n",
    "    print('============================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b66e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Classification ML Algorithm Evaluation Matrix SVC() is\n",
    "\n",
    "\n",
    "Accuracy Score :\n",
    " 0.9019607843137255\n",
    "\n",
    "\n",
    "Confusion matrix : \n",
    " [[342  26]\n",
    " [ 49 348]]\n",
    "\n",
    "\n",
    "Classification Report : \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.87      0.93      0.90       368\n",
    "           1       0.93      0.88      0.90       397\n",
    "\n",
    "    accuracy                           0.90       765\n",
    "   macro avg       0.90      0.90      0.90       765\n",
    "weighted avg       0.90      0.90      0.90       765\n",
    "\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "Classification ML Algorithm Evaluation Matrix GaussianNB() is\n",
    "\n",
    "\n",
    "Accuracy Score :\n",
    " 0.8470588235294118\n",
    "\n",
    "\n",
    "Confusion matrix : \n",
    " [[315  53]\n",
    " [ 64 333]]\n",
    "\n",
    "\n",
    "Classification Report : \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.86      0.84       368\n",
    "           1       0.86      0.84      0.85       397\n",
    "\n",
    "    accuracy                           0.85       765\n",
    "   macro avg       0.85      0.85      0.85       765\n",
    "weighted avg       0.85      0.85      0.85       765\n",
    "\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "Classification ML Algorithm Evaluation Matrix DecisionTreeClassifier() is\n",
    "\n",
    "\n",
    "Accuracy Score :\n",
    " 0.803921568627451\n",
    "\n",
    "\n",
    "Confusion matrix : \n",
    " [[294  74]\n",
    " [ 76 321]]\n",
    "\n",
    "\n",
    "Classification Report : \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.79      0.80      0.80       368\n",
    "           1       0.81      0.81      0.81       397\n",
    "\n",
    "    accuracy                           0.80       765\n",
    "   macro avg       0.80      0.80      0.80       765\n",
    "weighted avg       0.80      0.80      0.80       765\n",
    "\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "Classification ML Algorithm Evaluation Matrix KNeighborsClassifier(n_neighbors=22) is\n",
    "\n",
    "\n",
    "Accuracy Score :\n",
    " 0.8379084967320262\n",
    "\n",
    "\n",
    "Confusion matrix : \n",
    " [[270  98]\n",
    " [ 26 371]]\n",
    "\n",
    "\n",
    "Classification Report : \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.91      0.73      0.81       368\n",
    "           1       0.79      0.93      0.86       397\n",
    "\n",
    "    accuracy                           0.84       765\n",
    "   macro avg       0.85      0.83      0.84       765\n",
    "weighted avg       0.85      0.84      0.84       765\n",
    "\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "Classification ML Algorithm Evaluation Matrix RandomForestClassifier() is\n",
    "\n",
    "\n",
    "Accuracy Score :\n",
    " 0.8980392156862745\n",
    "\n",
    "\n",
    "Confusion matrix : \n",
    " [[342  26]\n",
    " [ 52 345]]\n",
    "\n",
    "\n",
    "Classification Report : \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.87      0.93      0.90       368\n",
    "           1       0.93      0.87      0.90       397\n",
    "\n",
    "    accuracy                           0.90       765\n",
    "   macro avg       0.90      0.90      0.90       765\n",
    "weighted avg       0.90      0.90      0.90       765\n",
    "\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "Classification ML Algorithm Evaluation Matrix AdaBoostClassifier() is\n",
    "\n",
    "\n",
    "Accuracy Score :\n",
    " 0.8457516339869281\n",
    "\n",
    "\n",
    "Confusion matrix : \n",
    " [[311  57]\n",
    " [ 61 336]]\n",
    "\n",
    "\n",
    "Classification Report : \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.85      0.84       368\n",
    "           1       0.85      0.85      0.85       397\n",
    "\n",
    "    accuracy                           0.85       765\n",
    "   macro avg       0.85      0.85      0.85       765\n",
    "weighted avg       0.85      0.85      0.85       765\n",
    "\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "Classification ML Algorithm Evaluation Matrix GradientBoostingClassifier() is\n",
    "\n",
    "\n",
    "Accuracy Score :\n",
    " 0.8562091503267973\n",
    "\n",
    "\n",
    "Confusion matrix : \n",
    " [[320  48]\n",
    " [ 62 335]]\n",
    "\n",
    "\n",
    "Classification Report : \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.87      0.85       368\n",
    "           1       0.87      0.84      0.86       397\n",
    "\n",
    "    accuracy                           0.86       765\n",
    "   macro avg       0.86      0.86      0.86       765\n",
    "weighted avg       0.86      0.86      0.86       765\n",
    "\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "Classification ML Algorithm Evaluation Matrix BaggingClassifier() is\n",
    "\n",
    "\n",
    "Accuracy Score :\n",
    " 0.873202614379085\n",
    "\n",
    "\n",
    "Confusion matrix : \n",
    " [[332  36]\n",
    " [ 61 336]]\n",
    "\n",
    "\n",
    "Classification Report : \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.90      0.87       368\n",
    "           1       0.90      0.85      0.87       397\n",
    "\n",
    "    accuracy                           0.87       765\n",
    "   macro avg       0.87      0.87      0.87       765\n",
    "weighted avg       0.88      0.87      0.87       765\n",
    "\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "We can see that RandomForestClassifier() gives us good Accuracy and maximum f1 score. so we will continue further investigation with crossvalidation of above model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af59a8c",
   "metadata": {},
   "source": [
    "## CrossValidation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c38e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "model=[LogisticRegression(),\n",
    "        SVC(),\n",
    "        GaussianNB(),\n",
    "        DecisionTreeClassifier(),\n",
    "        KNeighborsClassifier(n_neighbors = 12),\n",
    "        RandomForestClassifier(),\n",
    "        AdaBoostClassifier(),\n",
    "        GradientBoostingClassifier(),\n",
    "        BaggingClassifier()]\n",
    "\n",
    "for m in model:\n",
    "    score = cross_val_score(m, X, Y, cv =5)\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+'Cross Validation Score', m, ':'+'\\033[0m\\n')\n",
    "    print(\"Score :\" ,score)\n",
    "    print(\"Mean Score :\",score.mean())\n",
    "    print(\"Std deviation :\",score.std())\n",
    "    print('\\n')\n",
    "    print('============================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d87969",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cross Validation Score LogisticRegression() :\n",
    "\n",
    "Score : [0.66810345 0.69114471 0.66954644 0.70194384 0.70410367]\n",
    "Mean Score : 0.686968421836598\n",
    "Std deviation : 0.01545790574091748\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "\n",
    "\n",
    "Cross Validation Score SVC() :\n",
    "\n",
    "Score : [0.58405172 0.62419006 0.60475162 0.60043197 0.62419006]\n",
    "Mean Score : 0.6075230878081477\n",
    "Std deviation : 0.015260709074080395\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "\n",
    "\n",
    "Cross Validation Score GaussianNB() :\n",
    "\n",
    "Score : [0.67241379 0.76457883 0.7300216  0.76025918 0.77537797]\n",
    "Mean Score : 0.7405302748193938\n",
    "Std deviation : 0.03723496223050999\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "\n",
    "\n",
    "Cross Validation Score DecisionTreeClassifier() :\n",
    "\n",
    "Score : [0.67672414 0.86825054 0.90064795 0.86825054 0.87688985]\n",
    "Mean Score : 0.8381526029641766\n",
    "Std deviation : 0.08158083516724524\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "\n",
    "\n",
    "Cross Validation Score KNeighborsClassifier(n_neighbors=12) :\n",
    "\n",
    "Score : [0.72198276 0.71274298 0.77105832 0.74730022 0.73434125]\n",
    "Mean Score : 0.7374851046399047\n",
    "Std deviation : 0.020424867823627377\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "\n",
    "\n",
    "Cross Validation Score RandomForestClassifier() :\n",
    "\n",
    "Score : [0.70689655 0.97192225 0.95896328 0.96760259 0.98056156]\n",
    "Mean Score : 0.9171892455500112\n",
    "Std deviation : 0.10537679415558601\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "\n",
    "\n",
    "Cross Validation Score AdaBoostClassifier() :\n",
    "\n",
    "Score : [0.60991379 0.93736501 0.92440605 0.92224622 0.96544276]\n",
    "Mean Score : 0.8718747672599985\n",
    "Std deviation : 0.13188200240296455\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "\n",
    "\n",
    "Cross Validation Score GradientBoostingClassifier() :\n",
    "\n",
    "Score : [0.56681034 0.96760259 0.96328294 0.94384449 0.97408207]\n",
    "Mean Score : 0.8831244879719968\n",
    "Std deviation : 0.15847824171360397\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "\n",
    "\n",
    "Cross Validation Score BaggingClassifier() :\n",
    "\n",
    "Score : [0.65517241 0.93088553 0.94816415 0.9287257  0.96544276]\n",
    "Mean Score : 0.8856781112683398\n",
    "Std deviation : 0.11601429810714531\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "On basis of maximum score in crossvalidation of Random Forest Classifier. we will apply Hyperparameter tuning on Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73719ef",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning : GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02de5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1a7e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = {  'bootstrap': [True], 'max_depth': [5, 10,20,40,50, None], \n",
    "              'max_features': ['auto', 'log2'], \n",
    "              'criterion':['gini','entropy'],\n",
    "              'n_estimators': [5, 10, 15 ,25,50,100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9840cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV = GridSearchCV(RandomForestClassifier(),parameter,cv=5,n_jobs = -1,verbose=3)\n",
    "GCV.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250b16f",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
    "GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
    "             param_grid={'bootstrap': [True], 'criterion': ['gini', 'entropy'],\n",
    "                         'max_depth': [5, 10, 20, 40, 50, None],\n",
    "                         'max_features': ['auto', 'log2'],\n",
    "                         'n_estimators': [5, 10, 15, 25, 50, 100]},\n",
    "             verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a11209",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215fc2c6",
   "metadata": {},
   "source": [
    "{'bootstrap': True,\n",
    " 'criterion': 'entropy',\n",
    " 'max_depth': 20,\n",
    " 'max_features': 'log2',\n",
    " 'n_estimators': 25}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131853a3",
   "metadata": {},
   "source": [
    "## Final Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b93b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_mod = RandomForestClassifier(bootstrap=True,criterion='entropy',n_estimators= 25, max_depth=20 ,max_features='log2')\n",
    "Final_mod.fit(X_train,Y_train)\n",
    "y_pred=Final_mod.predict(X_test)\n",
    "print('\\033[1m'+'Accuracy Score :'+'\\033[0m\\n', accuracy_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44bd83e",
   "metadata": {},
   "source": [
    "Accuracy Score :\n",
    " 0.8915032679738563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dadce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "y_pred_prob = Final_mod.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(Y_test,y_pred_prob)\n",
    "plt.plot([0,1],[0,1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Random Forest Classifier')\n",
    "plt.xlabel('False postive rate')\n",
    "plt.ylabel('True postive rate')\n",
    "plt.show()\n",
    "auc_score = roc_auc_score(Y_test, Final_mod.predict(X_test))\n",
    "print('\\033[1m'+'Auc Score :'+'\\033[0m\\n',auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae11161",
   "metadata": {},
   "source": [
    "Auc Score :\n",
    " 0.8919922516701346"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576385e",
   "metadata": {},
   "source": [
    "## Saving model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f022ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(Final_mod,'IBM_HR_Analytics_Final.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
